{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import torchvision.transforms.functional as TF\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation, rc\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "\n",
    "sns.set_theme(style=\"white\")\n",
    "sns.set_context(\"talk\")\n",
    "figsize = (256 / 30, 128 / 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from krcps.utils import _split_idx\n",
    "\n",
    "n = 512\n",
    "gt = Image.open(os.path.join(\"assets\", \"ground_truth.jpg\"))\n",
    "gt = TF.to_tensor(gt)\n",
    "x = torch.rand(n, *gt.size())\n",
    "\n",
    "m = 128\n",
    "M = (torch.mean(gt, dim=0) >= 0.5).long()\n",
    "mu = x + 0.2 * torch.randn_like(x) * (1 - M) + 0.8 * torch.randn_like(x) * M\n",
    "mu = mu.unsqueeze(1)\n",
    "mu = mu.repeat(1, m, 1, 1, 1)\n",
    "y = torch.randn_like(mu) * 0.1 + mu\n",
    "\n",
    "n_val = 128\n",
    "val_idx, cal_idx = _split_idx(n, n_val)\n",
    "\n",
    "cal_x, cal_y = x[cal_idx], y[cal_idx]\n",
    "val_x, val_y = x[val_idx], y[val_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from krcps.utils import get_uq\n",
    "from krcps.utils import get_calibration\n",
    "\n",
    "m_cal_x, m_cal_y = torch.mean(cal_x, dim=1), torch.mean(cal_y, dim=2)\n",
    "m_val_x, m_val_y = torch.mean(val_x, dim=1), torch.mean(val_y, dim=2)\n",
    "\n",
    "alpha = 0.10\n",
    "calibrated_quantile_fn = get_uq(\"calibrated_quantile\", alpha=alpha, dim=1)\n",
    "m_cal_i = calibrated_quantile_fn(m_cal_y)\n",
    "\n",
    "rcps_fn = get_calibration(\"rcps\")\n",
    "krcps_fn = get_calibration(\"k_rcps\")\n",
    "\n",
    "epsilon = delta = 0.10\n",
    "lambda_max = 0.5\n",
    "stepsize = 2e-03\n",
    "_lambda = rcps_fn(\n",
    "    m_cal_x, m_cal_i, \"01\", \"hoeffding_bentkus\", epsilon, delta, lambda_max, stepsize\n",
    ")\n",
    "\n",
    "k = 2\n",
    "n_opt = 128\n",
    "prob_size = 50\n",
    "gamma = np.linspace(0.25, 0.75, 16)\n",
    "_lambda_k = krcps_fn(\n",
    "    m_cal_x,\n",
    "    m_cal_i,\n",
    "    \"hoeffding_bentkus\",\n",
    "    epsilon,\n",
    "    delta,\n",
    "    lambda_max,\n",
    "    stepsize,\n",
    "    k,\n",
    "    \"01_loss_otsu\",\n",
    "    n_opt,\n",
    "    prob_size,\n",
    "    gamma,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_i = calibrated_quantile_fn(m_val_y)\n",
    "\n",
    "_lambda_l, _lambda_u = val_i(_lambda)\n",
    "rcps_mu_i = torch.mean(_lambda_u - _lambda_l)\n",
    "print(f\"RCPS, mean interval length: {rcps_mu_i:.4f}\")\n",
    "\n",
    "_lambda_k_l, _lambda_k_u = val_i(_lambda_k)\n",
    "k_rcps_mu_i = torch.mean(_lambda_k_u - _lambda_k_l)\n",
    "print(f\"K-RCPS, mean interval length: {k_rcps_mu_i:.4f}\")\n",
    "print(\n",
    "    f\"K-RCPS reduces the mean interval length by {100 * (rcps_mu_i - k_rcps_mu_i) / rcps_mu_i:.2f}%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc(\"animation\", html=\"html5\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(2 * figsize[0], figsize[1]))\n",
    "ax = axes[1]\n",
    "ax.axis(\"off\")\n",
    "ax.set_title(r\"$K$-RCPS calibration results ($\\lambda_K,~K=2$)\")\n",
    "im = ax.imshow(_lambda_k, cmap=\"jet\")\n",
    "\n",
    "ax = axes[0]\n",
    "samples = val_y[0]\n",
    "vmin, vmax = torch.quantile(samples, torch.tensor([0.01, 0.99]))\n",
    "samples = (samples - vmin) / (vmax - vmin)\n",
    "samples = torch.clamp(samples, 0, 1)\n",
    "\n",
    "ax.axis(\"off\")\n",
    "ax.set_title(\"Samples from a diffusion model\")\n",
    "im = ax.imshow(torch.zeros_like(gt).permute(1, 2, 0), cmap=\"gray\", vmin=vmin, vmax=vmax)\n",
    "\n",
    "\n",
    "def _init():\n",
    "    im.set_data(torch.zeros_like(gt).permute(1, 2, 0))\n",
    "    return (im,)\n",
    "\n",
    "\n",
    "def _animate(i):\n",
    "    im.set_data(samples[i].permute(1, 2, 0))\n",
    "    return (im,)\n",
    "\n",
    "\n",
    "anim = animation.FuncAnimation(fig, _animate, frames=m, init_func=_init)\n",
    "anim.save(os.path.join(\"assets\", \"results.gif\"), writer=animation.PillowWriter(fps=60))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "6eca7a80bcca63028e18e993a5273015075fda56665fa034661a2325c6291851"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
